---
title: "Découverte du Datalab"
description: "TP3 - Onyxia et le Datalab pour le statisticien"
author: "Ludovic Deneuville"
format: 
  html:
    toc: true
    toc-location: left
    toc-expand: 3
from: markdown+emoji
number-sections: true
number-depth: 3
lightbox: true
---


## Introduction {.unnumbered}

Vous allez effectuer une partie des TP sur le [Datalab du GENES](https://onyxia.lab.groupe-genes.fr/){target="_blank"}.

Le Datalab permet aux statisticiens de découvrir, d'expérimenter, d'apprendre, de se former aux outils de la data.

::: {.callout-tip title="Qu'est-ce que le Datalab ?" collapse="true"}
Dans le monde professionnel, plusieurs problèmes se posent au statisticien :

- sa machine n'est pas assez puissante
- installer des logiciels est parfois compliqué car bloqué par le système

À l'INSEE, un projet est né pour pallier à ce besoin : [Onyxia](https://www.onyxia.sh/){target="_blank"} porté par le [lab de l'INSEE](https://github.com/InseeFrLab){target="_blank"}.

La première étape fut de créer une infrastructure Cloud avec des CPUs, des GPUs, de la RAM et du stockage.

Ensuite, c'est très bien d'avoir accès à tout plein de ressources, encore faut-il savoir les utiliser. Or lancer des services dans le cloud nécessite quelques compétences spécifiques (Docker, Kubernetes, Helm, S3...) qui ne sont pas à la portée de tous.

D'où l'idée de créer une interface graphique pour pouvoir lancer des services en quelques clics. Onyxia s'appuie sur des technologies sous-jacentes (Kubernetes, Helm, S3...) pour proposer une IHM agréable pour le datascientist. Cependant la philosophie du projet Onyxia est d'être une brique qui facilite le travail sans se rendre indispensable.

À l'INSEE, une instance de ce logiciel a été installée sur le [Datalab SSP Cloud](https://datalab.sspcloud.fr/){target="_blank"} ouverte à tous les agents publics. Vous pouvez y créer un compte car c'est également ouvert à tous les élèves de l'ENSAI (en utilisant votre mail ENSAI).

Onyxia est Libre et Open Source, ainsi chacun peut installer sa propre instance sur son infra. C'est ce qu'ont fait de nombreux organismes, ainsi que le GENES.

Pour plus de détails, voir la section **Pour aller plus loin** à la fin.
:::

Le principe est d'offrir aux utilisateurs de nombreux services (Jupyter, Rstudio, VSCode, PostgreSQL...) à la demande et une puissance de calcul adaptée aux besoins.

::: {.callout-caution}
Ces services sont éphémères, car si les ressources sont importantes, il faut savoir les partager !

Le principe général est le suivant :

- Vous lancez votre service en réservant des ressources (CPU, GPU, RAM)
- Vous effectuez votre travail
- Vous sauvegardez votre code (git) et vos résultats (MinIO)
- Vous supprimez votre service et libérez les ressources 
:::


## Objectifs {.unnumbered}

Ce TP d'initiation va vous permettre de :

- Configurer votre compte sur le datalab
- Vous familiariser avec l'espace de stockage S3
- Lancer vos premiers services


## Créer et configuration de compte

::: {.callout-important}
Sur le datalab, vos services ont une [**durée de vie limitée**]{.underline}.

Pour sauvegarder vos programmes, la bonne pratique est d'utiliser un dépôt git. Nous allons donc créer et utiliser un jeton pour communiquer avec GitHub.

Pour suivre la démarche, il faut disposer d'un compte [GitHub](https://github.com/){target="_blank"}.
Il est possible de suivre une démarche similaire avec [GitLab](https://about.gitlab.com/){target="_blank"}.
:::


### Générer un token GitHub

::: {.callout-tip title="Déjà fait ?"}
Si vous avez déjà généré et déclaré un jeton GitHub, inutile de refaire ces 2 étapes.
:::

- [ ] Connectez-vous à votre compte GitHub
- [ ] Allez dans settings :arrow_right: Developer settings :arrow_right: Personal access tokens :arrow_right: Tokens (classic)
- [ ] Générez un [nouveau jeton classique](https://github.com/settings/tokens/new){target="_blank"}
  - Renseigner : 
    - nom du token : Datalab GENES
    - date d'expiration :arrow_right: Custom :arrow_right: 1 an
  - :white_check_mark: Cochez repo
  - Cliquez sur [Generate token]{.green-button}
  - Copiez ce jeton commençant par `ghp_` et gardez le précieusement de côté quelques minutes

::: {.callout-warning}
- Ce jeton ne sera visible qu'une seule fois
- si vous le perdez ou s'il est expiré, il faut en générer un nouveau
:::


### Déclarer votre jeton

GitHub vous a fournit un jeton. Il faut maintenant le déclarer sur le [Datalab](https://onyxia.lab.groupe-genes.fr/account/git){target="_blank"} :

- [ ] Allez dans `Mon Compte` :arrow_right: Onglet `Git`
- [ ] Renseignez les informations suivantes 
  - nom d'utilisateur Git 
  - mail (celui utilisé pour votre compte GitHub)
- [ ] Collez votre token

::: {.callout-tip title="Config Git" }
Vous pouvez maintenant échanger du code entre les services du Datalab et vos dépôts GitHub. :tada:
:::

## Le stockage

Lorsque l'on travaille dans le cloud, il est essentiel de [séparer les données des programmes]{.underline} pour :

- mieux gérer les ressources, 
- renforcer la sécurité en limitant les accès et les permissions,
- permettre une scalabilité indépendante des composants.

[MinIO](https://min.io/){target="_blank"} est une solution de stockage d'objets open-source qui permet de déployer facilement un stockage évolutif et performant. Elle est compatible avec l'API [S3 d'Amazon](https://aws.amazon.com/fr/s3/){target="_blank"}, ce qui facilite l'intégration avec les applications existantes. 

MinIO offre :

- une haute disponibilité, 
- une sécurité renforcée grâce au chiffrement des données et des contrôles d'accès,
- des performances élevées, particulièrement adaptées aux environnements nécessitant un accès rapide aux données, comme le Big Data et l'intelligence artificielle.

### Votre bucket

::: {.callout-note title="Définition"}
Un **bucket** est un conteneur de stockage utilisé pour regrouper des objets (fichiers et métadonnées) dans des systèmes de stockage de type cloud. Il facilite l'organisation, la gestion des permissions et l'accès aux données dans un espace de stockage structuré.
:::

Lors de votre création de compte, un bucket est créé avec votre nom d'utilisateur. Dans ce bucket, vous pouvez :

- créer / supprimer des dossiers
- importer / supprimer des fichiers
- créer un dossier nommé *diffusion* à la racine de votre bucket
  - celui-ci sera accessible en lecture à tous les utilisateurs du datalab


Pour accéder à votre stockage :

- Depuis le Datalab, allez dans `Mes fichiers`
- Depuis la [console MinIO](https://minio-console.lab.sspcloud.fr/){target="_blank"}
- Depuis le terminal d'un service (voir [section ci-après](#Accéder-à-votre-stockage))


### Stocker des fichiers

- [ ] Téléchargez le [fichier des prénoms par département de naissance](https://www.data.gouv.fr/fr/datasets/base-prenoms-insee-format-parquet/){target="_blank"} en 2022 au format parquet
- [ ] Dans votre bucket, créez un dossier `diffusion`, puis à l'intérieur un dossier `INSEE`
- [ ] Dans ce dossier, importez le fichier des prénoms

::: {.callout-note}
Nous allons ensuite utiliser ce fichier dans un service.
:::

## Les services

### Dépôt pour le code

Avant de créer un service, nous allons créer un dépôt GitHub qui permettra de sauvegarder votre code.

- [ ] Dans GitHub, créer un [nouveau Repository](https://github.com/new){target="_blank"}
  - Repository name : TP3-datalab
  - Public
  - :white_check_mark: Cochez *Add a README file*
  - .gitignore template : Python
  - Choose a license : Apache Licence 2.0
  - [Create Repository]{.green-button}
- [ ] Sur la page de votre repo, cliquez sur le bouton [Code]{.green-button}
- [ ] Copiez l'adresse *https* du repo

### Créer un service

- [ ] Allez dans `Catalogue de services`, sélectionner le service *Jupyter-python*, puis cliquez une fois sur `Lancer`
- [ ] Ouvrez la Configuration
  - De nombreux onglets permettent de configurer votre service
  - Service : possibilité d'utiliser une autre image Docker
  - Resources : choisir CPU et RAM
  - Init : script à lancer au démarrage
- [ ] Allez dans l'onglet `Git` et collez l'adresse *https* du repo dans la case *Repository url*
- [ ] `Lancez` le service, puis attendez quelques secondes
- [ ] Cliquez pour copier le mot de passe
- [ ] Cliquez sur `Ouvrir le service` :rocket:
  - password : collez le mot de passe

Votre service *Jupyter Python* s'ouvre.

Vous remarquez dans l'explorateur à gauche que votre repo git a été cloné dans le dossier *work*.

::: {.callout-note collapse="true" title="Ce qu'il se passe sous le capot"}

Pour plus de détails, vous pouvez consulter la vidéo en bas de la page.

Prenons l'exemple de ce qu'il se passe en arrière plan lorsque vous lancez un service `Vscode-python` :

#### Construction de l'image Docker

Une image Docker est un paquet léger et autonome qui contient tout le nécessaire pour exécuter une application : le code, les bibliothèques, les dépendances, les variables d'environnement et les fichiers de configuration.

Les images Docker permettent de créer des environnements isolés et cohérents, garantissant ainsi la portabilité et la reproductibilité des applications.

1. Nous partons de l'[image de base d'Onyxia](https://github.com/InseeFrLab/images-datascience/blob/main/base/Dockerfile){target="_blank"}
    - ubuntu:22.04 et quelques éléments de config
2. Nous ajoutons la couche [python-minimal](https://github.com/InseeFrLab/images-datascience/blob/main/python-minimal/Dockerfile){target="_blank"}
    - installation de python et quelques packages classiques
3. Nous ajoutons la couche [python-datascience](https://github.com/InseeFrLab/images-datascience/blob/main/python-datascience/Dockerfile){target="_blank"}
    - Julia, Quarto et des packages de datascience (numpy, pandas...)
4. Nous ajoutons la couche [vscode](https://github.com/InseeFrLab/images-datascience/blob/main/vscode/Dockerfile){target="_blank"}
    - Installation de Visual Studio Code et configuration

#### DockerHub

Cette image est construite et déposée sur DockerHub [onyxia-vscode-python](https://hub.docker.com/r/inseefrlab/onyxia-vscode-python){target="_blank"}.

Nous allons ensuite lancer une instance de cette image : un conteneur. Le conteneur est à l'image, ce que l'objet est à la classe.

Pour gérer tous les conteneurs lancés sur le datalab, nous avons besoin d'un orchestrateur : Kubernetes. 

#### Chart Helm

Cependant, nous allons d'abord utiliser Helm pour faciliter le déploiement.

Helm simplifie le processus de déploiement d'applications sur Kubernetes en automatisant les tâches répétitives et en fournissant une gestion centralisée des configurations.

Le [chart Helm vscode-python](https://github.com/InseeFrLab/helm-charts-interactive-services/tree/main/charts/vscode-python){target="_blank"} est un ensemble de fichiers de configuration qui facilite le déploiement d'application dans un envrionnement Kubernetes.

#### Déploiement sur Kube

Kubernetes (K8s) est un système open-source qui automatise le déploiement, la mise à l'échelle et la gestion d'applications conteneurisées. C'est un orchestrateur de conteneurs.

K8s récupére via le chart Helm toutes les infos nécessaires et déploie le conteneur (créé à partir de l'image Docker) sur l'infra du datalab.

:::

### Jouer avec votre service

- [ ] Dans l'explorateur, ouvrez le dossier *work*, puis le dossier *TP3-Datalab*
- [ ] File :arrow_right: New :arrow_right: Notebook
  - Select Kernel
  - Cela crée un fichier *Untitled.ipynb* que vous pouvez renommer en *ex0.ipynb*


### Accéder à votre stockage

- [ ] Créez un dataframe Polars en important les données de votre fichier
  - Collez le code ci-dessous dans la première cellule, puis CTRL+ENTREE pour exécuter
  ```{.python}
  import os
  import polars as pl
  
  storage_options = {
      "aws_endpoint":  'https://'+'minio-simple.lab.groupe-genes.fr',
      "aws_access_key_id": os.environ["AWS_ACCESS_KEY_ID"],
      "aws_secret_access_key": os.environ["AWS_SECRET_ACCESS_KEY"],
      "aws_region": os.environ["AWS_DEFAULT_REGION"],
      "aws_token": os.environ["AWS_SESSION_TOKEN"]
    }
  username = os.environ["VAULT_TOP_DIR"] # un peu bancal pour avoir le username
  
  df = pl.read_parquet(source=f"s3://{username}/diffusion/INSEE/prenoms-nat2022.parquet", 
                       storage_options=storage_options)
  print(df)
  ```

::: {.callout-note title="Polars"}
[Polars](https://pola.rs/){target="_blank"} est une bibliothèque rapide et performante pour la manipulation de données en Python et Rust, optimisée pour les grands volumes de données. 

Contrairement à [Pandas](https://pandas.pydata.org/){target="_blank"}, Polars utilise une approche orientée colonnes (columnar) et tire parti du traitement parallèle, ce qui le rend particulièrement adapté pour les opérations analytiques lourdes. 

Sa syntaxe, proche de celle de Pandas, permet des transformations, agrégations et manipulations de données avec des méthodes performantes et expressives.
:::

- [ ] Requêtez les données
  - Créez une nouvelle cellule avec le bouton `+` de la barre d'outils
  - Collez et exécutez ce code
  ```{.python}
  # Top 10 des prénoms féminins en 2021
  top10f2021 = df\
      .filter((pl.col("sexe") == 2) & 
              (pl.col("annais") == "2021") & 
              (pl.col("preusuel") != "_PRENOMS_RARES"))\
      .group_by("preusuel")\
      .agg(pl.col("nombre").sum().alias("nombre_total"))\
      .sort("nombre_total", descending=True)\
      .limit(10)
  
  print(top10f2021)
  ```

### Jouer avec vos données

- [ ] Générez le tableau du nombre de fois où votre prénom a été donné entre 2010 et 2020
  ```{.python}
  prenom = "Ludovic"
  annee_debut, annee_fin = 2010, 2020
  
  df_prenom_annee = df\
      .filter((pl.col("annais") != "XXXX"))\
      .filter((pl.col("preusuel") == prenom.upper()) & 
              (pl.col("annais").cast(pl.Int32) >= annee_debut) & 
              (pl.col("annais").cast(pl.Int32) <= annee_fin))\
      .group_by("annais")\
      .agg(pl.col("nombre").sum().alias("nombre_total"))\
      .sort("annais")
  
  print(df_prenom_annee)
  ```
- [ ] Faites un diagramme en barres
  - avec les librairies *matplotlib* et *seaborn*
  ```{.python}
  import matplotlib.pyplot as plt
  import seaborn as sns
  
  # Set seaborn style for a cleaner, more appealing look
  sns.set_theme(style="whitegrid")
  
  # Create the plot
  fig, ax = plt.subplots(figsize=(10, 6))
  
  # Data
  years = df_prenom_annee["annais"].to_list()         # Extracts years as a list
  counts = df_prenom_annee["nombre_total"].to_list()  # Extracts counts as a list
  
  # Bar plot with custom colors and transparency for better visuals
  ax.bar(years, 
         counts, 
         color="#4C72B0", 
         edgecolor="black", 
         alpha=0.8)
  
  # Set labels and title with padding for readability
  ax.set_xlabel("Année", fontsize=12, labelpad=10)
  ax.set_ylabel("Nombre d'occurences", fontsize=12, labelpad=10)
  ax.set_title(f"Occurrences du prénom {prenom} par année ({annee_debut}-{annee_fin})", fontsize=14, pad=15)
  
  # Rotate x-axis labels, improve spacing, and format grid
  ax.tick_params(axis='x', rotation=45)
  plt.tight_layout()  # Adjust layout for tight fit
  
  # Display the plot
  plt.show()
  ```



### Exportez vos résultats vers MinIO

- [ ] En utilisant la librairie *s3fs*
  ```{.python}
  import s3fs
  
  fs = s3fs.S3FileSystem(
      client_kwargs={'endpoint_url': 'https://'+'minio-simple.lab.groupe-genes.fr'},
      key = os.environ["AWS_ACCESS_KEY_ID"], 
      secret = os.environ["AWS_SECRET_ACCESS_KEY"], 
      token = os.environ["AWS_SESSION_TOKEN"])
  
  destination = f"s3://{username}/diffusion/INSEE/output.csv"
  
  with fs.open(destination, mode='wb') as f:
      top10f2021.write_csv(f)
  ```
- [ ] Sur le Datalab, allez dans Mes fichiers :arrow_right: diffusion :arrow_right: INSEE
  - le fichier *output.csv* a été généré
  - rafraichissez la page si besoin
- [ ] Double-cliquez sur ce fichier pour en avoir un aperçu


#### Exercice

::: {.callout-note}
Si vous êtes en avance.
:::

Comme dans de très nombreux outils, il est possible d'encapsuler du [SQL dans Polars](https://docs.pola.rs/api/python/stable/reference/sql/index.html){target="_blank"}.

Par exemple la requête du top10 féminin 2021 donne en SQL :

```{.python}
df.sql("""
  SELECT preusuel, 
         SUM(nombre) AS nombre_total
    FROM self
   WHERE preusuel <> '_PRENOMS_RARES'
     AND annais = '2021'
     AND sexe = 2
   GROUP BY preusuel
   ORDER BY 2 DESC
   LIMIT 10
""")
```

- [ ] Ré-écrivez en SQL la requête du nombre de fois ou votre prénom a été donné entre 2010 et 2020
- [ ] Créez une nouvelle cellule et écrivez une requête pour afficher les 5 années où sont nées le plus de filles
- [ ] Quels sont les prénoms masculins et féminins les plus donnés depuis 1900 ?
- [ ] Affichez le nombres de filles et de garçons nés chaque année depuis 2010
  - :bulb: Aide : `SUM(CASE WHEN ? THEN ? ELSE ? END)`


### Client MinIO

::: {.callout-caution}
Ne pas faire cette partie.
:::

Le [client MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html){target="_blank"} installé et [utilisable depuis le terminal]{.underline} permet également d'interagir avec vos fichiers.

Ouvrez un terminal :

- [ ] `mc ls s3/<username>/diffusion` : pour lister le contenu de votre dossier diffusion
- [ ] `mc cp s3/<username>/diffusion/INSEE/prenoms-nat2022.parquet .` : pour copier le fichier depuis s3 dans votre dossier courant
  - le fichier apparait dans votre explorer
- [ ] Supprimez ce fichier : `rm prenoms-nat2022.parquet`
  - Car importer les fichiers de données dans son espace de travail n'est pas une bonne pratique

::: {.callout-tip}
La commande `mc --help` liste toutes les commandes possibles (ESPACE pour défiler, CTRL+C pour sortir)
:::


### Sauver son code

- [ ] Sur Jupyter, ouvrez un terminal
  - File :arrow_right: New :arrow_right: Terminal
- [ ] Positionnez-vous dans le repo : `cd /home/onyxia/work/TP3-datalab/`
- [ ] `git status` pour voir l'état actuel
  - le fichier *ex0.ipynb* doit apparaître dans les *Untracked files*
- [ ] Ajoutez ce fichier à l'index
- [ ] Créez un commit
- [ ] Poussez ce commit vers votre dépôt distant (GitHub)
  - Vous pouvez vérifier sur GitHub que votre fichier *ex0.ipynb* est bien présent

::: {.callout-tip collapse="true"}
- `git add .`
- `git commit -m "création fichier tp3"`
- `git push`
:::

### Surveiller son service


::: {.callout-caution}
Ne pas faire cette partie.
:::

- [ ] Sur la page du Datalab, allez dans `Mes services`
- [ ] Cliquez sur le nom du service (Jupyter-python)
- [ ] Cliquez sur `Surveillance externe`

Vous arrivez sur la page de l'outil **Grafana** qui permet d'observer les métriques de votre service.

### Terminer son service

Une fois vos travaux terminés, il est temps de libérer les ressources réservées.

- [ ] Dans *Mes services*, mettez votre service à la poubelle

De toute manière, vous pouvez aisément reproduire votre travail plus tard :

- Votre code est sur GitHub
- Vos données sont sur MinIO
- Il suffit de relancer un nouveau service et d'y brancher votre répo


## Les secrets

Certains éléments ne doivent pas être diffusés dans votre code (jetons d'accès, mots de passe...).

Pour éviter d'avoir à nettoyer votre code à chaque fois que vous le poussez sur GitHub, le datalab propose de gérer vos secrets.

### Créer un secret

- [ ] Allez dans *Mes secrets*
- [ ] Créez un `Nouveau secret` nommé *projet_patate*
- [ ] Double-cliquez pour ouvrir ce secret
- [ ] `Ajoutez une variable`
  - Nom : PATATE_TOKEN
  - Valeur : 123456
  - Cliquez sur :white_check_mark: pour valider
- [ ] Ajoutez une autre variable
  - Nom : PATATE_PORT
  - Valeur : 5236
  - Cliquez sur :white_check_mark: pour valider

### Utiliser dans un service

- [ ] Préparez le lancement d'un service Rstudio
- [ ] Dans la configuration, allez dans l'onglet `Vault`
- [ ] secret : *projet_patate*
- [ ] Lancez le service

Dans votre servives, les deux variables d'environnement ont été créées. 

- [ ] Vérifiez leur présence via le terminal : `env | grep PATATE` ou `echo $PATATE_TOKEN`
- [ ] Ouvrez un notebook et récupérez la valeur de *PATATE_TOKEN*
  ```{.python}
  import os

  token = os.environ["PATATE_TOKEN"]
  print(token)
  ```
- [ ] Une fois que vous avez fini de jouer, supprimez votre service

::: {.callout-note}
Ce TP avait pour but de vous familiariser avec le Datalab. Vous pouvez l'utiliser pour faire du Python, du R, créer une base de données (prochain TP) ou utiliser de nombreux autres outils.
:::


## Pour aller plus loin {.unnumbered}

### Le projet Onyxia de A à Z {.unnumbered  .unlisted}

Une présentation complète du projet Onyxia au Devoxx 2023, de sa philosophie et de son fonctionnement par Olivier LEVITT, Joseph GARRONE et Frédéric COMTE.

{{< video https://www.youtube.com/watch?v=GXINfnVB21E 
    title="Construisons un cloud opensource pour le datascientist"
>}}

### Le projet Onyxia (version courte) {.unnumbered .unlisted}

{{< video https://www.youtube.com/watch?v=sOOVg4I19BI >}}



## Bibliographie {.unnumbered}

- [Utiliser RStudio sur l’environnement SSP Cloud](https://book.utilitr.org/01_R_Insee/Fiche_utiliser_Rstudio_SSPCloud.html){target="_blank"}, UtilitR
- [Découverte du Datalab](https://www.sspcloud.fr/formation){target="_blank"}, Plateforme SSP Cloud
- [Découverte d'Onyxia et de son datalab SSP Cloud](https://github.com/TheAIWizard/Hands-on-Spark-Lab/blob/main/First-steps-with-cloud-computing/First-steps-with-cloud-computing.md){target="_blank"}, Nathan Randriamanana